{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Assignment: Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
    "Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "\n",
    "In Notebook 4A, we built a sentiment analysis model for movie reviews.\n",
    "That particular sentiment analysis was a two-class classification problem, where the two classes were whether the review was positive or negative.\n",
    "Of course, natural language comes in all sorts of different forms; sometimes we want to perform other types of classification.\n",
    "\n",
    "For example, the AG News dataset contains text from 127600 online news articles, from 4 different categories: World, Sports, Business, and Science/Technology.\n",
    "AG News is typically used for topic classification: given an unseen news article, we're interested in predicting the topic.\n",
    "For this assignment, you'll be training several models on the AG News dataset.\n",
    "Unlike the quick example we trained in Notebook 4A, however, we're going to *learn* the word embeddings.\n",
    "Since you may be unfamiliar with AG News, we're going to walk through how to load the data, to get you started.\n",
    "\n",
    "\n",
    "### Loading AG News with Torchtext\n",
    "\n",
    "The AG News dataset is one of many included Torchtext.\n",
    "It can be found grouped together with many of the other text classification datasets.\n",
    "While we can download the source text online, Torchtext makes it retrievable with a quick API call&ast;. If you are running this notebook on your machine,  you can uncomment and run this block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_check_lambda_fn' from 'torch.utils.data.datapipes.utils.common' (/home/gergely/anaconda3/lib/python3.9/site-packages/torch/utils/data/datapipes/utils/common.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/gergely/Documents/machinelearning/4B_Natural_Language_Processing_Assignment.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gergely/Documents/machinelearning/4B_Natural_Language_Processing_Assignment.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchtext/__init__.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m _TEXT_BUCKET \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m _CACHE_DIR \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexpanduser(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(_get_torch_home(), \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m data, datasets, prototype, functional, models, nn, transforms, utils, vocab, experimental\n\u001b[1;32m     14\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__, git_version  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchtext/datasets/__init__.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mimportlib\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mag_news\u001b[39;00m \u001b[39mimport\u001b[39;00m AG_NEWS\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mamazonreviewfull\u001b[39;00m \u001b[39mimport\u001b[39;00m AmazonReviewFull\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mamazonreviewpolarity\u001b[39;00m \u001b[39mimport\u001b[39;00m AmazonReviewPolarity\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchtext/datasets/ag_news.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     _wrap_split_argument,\n\u001b[1;32m      8\u001b[0m     _create_dataset_directory,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[39mif\u001b[39;00m is_module_available(\u001b[39m\"\u001b[39m\u001b[39mtorchdata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m \u001b[39mimport\u001b[39;00m FileOpener, IterableWrapper\n\u001b[1;32m     13\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_download_hooks\u001b[39;00m \u001b[39mimport\u001b[39;00m HttpReader\n\u001b[1;32m     15\u001b[0m URL \u001b[39m=\u001b[39m {\n\u001b[1;32m     16\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mhttps://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mhttps://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchdata/__init__.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m \u001b[39mimport\u001b[39;00m _extension  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m datapipes\n\u001b[1;32m     11\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchdata/datapipes/__init__.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataChunk, functional_datapipe\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39miter\u001b[39m, \u001b[39mmap\u001b[39m, utils\n\u001b[1;32m     11\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mDataChunk\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfunctional_datapipe\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39miter\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmap\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mutils\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchdata/datapipes/iter/__init__.py:64\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39ms3io\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     56\u001b[0m     S3FileListerIterDataPipe \u001b[39mas\u001b[39;00m S3FileLister,\n\u001b[1;32m     57\u001b[0m     S3FileLoaderIterDataPipe \u001b[39mas\u001b[39;00m S3FileLoader,\n\u001b[1;32m     58\u001b[0m )\n\u001b[1;32m     59\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransform\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbucketbatcher\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     60\u001b[0m     BucketBatcherIterDataPipe \u001b[39mas\u001b[39;00m BucketBatcher,\n\u001b[1;32m     61\u001b[0m     InBatchShufflerIterDataPipe \u001b[39mas\u001b[39;00m InBatchShuffler,\n\u001b[1;32m     62\u001b[0m     MaxTokenBucketizerIterDataPipe \u001b[39mas\u001b[39;00m MaxTokenBucketizer,\n\u001b[1;32m     63\u001b[0m )\n\u001b[0;32m---> 64\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransform\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallable\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     65\u001b[0m     BatchMapperIterDataPipe \u001b[39mas\u001b[39;00m BatchMapper,\n\u001b[1;32m     66\u001b[0m     FlatMapperIterDataPipe \u001b[39mas\u001b[39;00m FlatMapper,\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     68\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbz2fileloader\u001b[39;00m \u001b[39mimport\u001b[39;00m Bz2FileLoaderIterDataPipe \u001b[39mas\u001b[39;00m Bz2FileLoader\n\u001b[1;32m     69\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcacheholder\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     70\u001b[0m     EndOnDiskCacheHolderIterDataPipe \u001b[39mas\u001b[39;00m EndOnDiskCacheHolder,\n\u001b[1;32m     71\u001b[0m     InMemoryCacheHolderIterDataPipe \u001b[39mas\u001b[39;00m InMemoryCacheHolder,\n\u001b[1;32m     72\u001b[0m     OnDiskCacheHolderIterDataPipe \u001b[39mas\u001b[39;00m OnDiskCacheHolder,\n\u001b[1;32m     73\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchdata/datapipes/iter/transform/callable.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Callable, Iterator, List, TypeVar\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m functional_datapipe, IterDataPipe\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m _check_lambda_fn\n\u001b[1;32m     12\u001b[0m T_co \u001b[39m=\u001b[39m TypeVar(\u001b[39m\"\u001b[39m\u001b[39mT_co\u001b[39m\u001b[39m\"\u001b[39m, covariant\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[39m@functional_datapipe\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmap_batches\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBatchMapperIterDataPipe\u001b[39;00m(IterDataPipe[T_co]):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_check_lambda_fn' from 'torch.utils.data.datapipes.utils.common' (/home/gergely/anaconda3/lib/python3.9/site-packages/torch/utils/data/datapipes/utils/common.py)"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "#agnews_train, agnews_test = torchtext.datasets.text_classification.DATASETS[\"AG_NEWS\"](root=\"./datasets\")\n",
    "#agnews_train, agnews_test = torchtext.datasets.AG_NEWS(root='./datasets', split=('train', 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"1\">&ast;At the time this notebook was created, Torchtext contains a small bug in its csv reader.\n",
    "You may need to change one line in the source code, as suggested [here](https://discuss.pytorch.org/t/one-error-about-the-utils-pys-code/53885) to successfully load the AG News dataset.\n",
    "</font>\n",
    "\n",
    "Unfortunately, Torchtext assumes we have network connectivity. If we don't have network access, such as notebooks running in Coursera Labs, we need to reimplement some Torchtext functionality. Skip this next block if you were able to successfully run the previous code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_log_class_usage' from 'torchtext.utils' (/home/gergely/anaconda3/lib/python3.9/site-packages/torchtext/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/gergely/Documents/machinelearning/4B_Natural_Language_Processing_Assignment.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gergely/Documents/machinelearning/4B_Natural_Language_Processing_Assignment.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gergely/Documents/machinelearning/4B_Natural_Language_Processing_Assignment.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ngrams \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gergely/Documents/machinelearning/4B_Natural_Language_Processing_Assignment.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_csv_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./datasets/ag_news_csv/train.csv\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchtext/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchtext/data/__init__.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset, TabularDataset\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexample\u001b[39;00m \u001b[39mimport\u001b[39;00m Example\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfield\u001b[39;00m \u001b[39mimport\u001b[39;00m RawField, Field, ReversibleField, SubwordField, NestedField, LabelField\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39miterator\u001b[39;00m \u001b[39mimport\u001b[39;00m (batch, BucketIterator, Iterator, BPTTIterator,\n\u001b[1;32m      6\u001b[0m                        pool)\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m bleu_score\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchtext/data/field.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipeline\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_tokenizer, dtype_to_attr, is_tokenizer_serializable\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvocab\u001b[39;00m \u001b[39mimport\u001b[39;00m Vocab, SubwordVocab\n\u001b[1;32m     14\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mRawField\u001b[39;00m(\u001b[39mobject\u001b[39m):\n\u001b[1;32m     15\u001b[0m     \u001b[39m\"\"\" Defines a general datatype.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39m    Every dataset consists of one or more types of data. For instance, a text\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m            Affects iteration over batches. Default: False\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchtext/vocab/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvectors\u001b[39;00m \u001b[39mimport\u001b[39;00m CharNGram, FastText, GloVe, pretrained_aliases, Vectors\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvocab\u001b[39;00m \u001b[39mimport\u001b[39;00m Vocab\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvocab_factory\u001b[39;00m \u001b[39mimport\u001b[39;00m build_vocab_from_iterator, vocab\n\u001b[1;32m      5\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mVocab\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mVectors\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchtext/vocab/vocab.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _log_class_usage\n\u001b[1;32m      8\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mVocab\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      9\u001b[0m     __jit_unused_properties__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mis_jitable\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_log_class_usage' from 'torchtext.utils' (/home/gergely/anaconda3/lib/python3.9/site-packages/torchtext/utils.py)"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "ngrams = 1\n",
    "train_csv_path = './datasets/ag_news_csv/train.csv'\n",
    "test_csv_path = './datasets/ag_news_csv/test.csv'\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    torchtext.datasets.text_classification._csv_iterator(train_csv_path, ngrams))\n",
    "train_data, train_labels = torchtext.datasets.text_classification._create_data_from_iterator(\n",
    "        vocab, torchtext.datasets.text_classification._csv_iterator(train_csv_path, ngrams, yield_cls=True), False)\n",
    "test_data, test_labels = torchtext.datasets.text_classification._create_data_from_iterator(\n",
    "        vocab, torchtext.datasets.text_classification._csv_iterator(test_csv_path, ngrams, yield_cls=True), False)\n",
    "if len(train_labels ^ test_labels) > 0:\n",
    "    raise ValueError(\"Training and test labels don't match\")\n",
    "agnews_train = torchtext.datasets.TextClassificationDataset(vocab, train_data, train_labels)\n",
    "agnews_test = torchtext.datasets.TextClassificationDataset(vocab, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the first data example to see how the data is formatted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(agnews_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Torchtext has each example as a tuple, with the first element being the label (0, 1, 2, or 3), and the second element the text data.\n",
    "Notice that the text is already \"tokenized\": the words of the news article have been represented as word IDs, with each number corresponding to a unique word.\n",
    "\n",
    "In previous notebooks, we've used `DataLoader`s to handle shuffling and batching.\n",
    "However, if we directly try to feed these dataset objects into a `DataLoader`, we will face an error when we try to draw our first batch.\n",
    "Can you figure out why?\n",
    "Here's a hint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Length of the first text example: {}\".format(len(agnews_train[0][1])))\n",
    "print(\"Length of the second text example: {}\".format(len(agnews_train[1][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each example is a news snippet, they can vary in length.\n",
    "This is natural, as humans don't stick to consistent sentence length while writing.\n",
    "This creates a bit of a problem while batching, as default tensors expect the size of each dimension to be consistent.\n",
    "\n",
    "How do we fix this?\n",
    "The common solution is to perform padding and/or truncation, picking a maximum sequence length $L$.\n",
    "Inputs longer than the maximum length are truncated (i.e. $x_{t>L}$ are discarded), and shorter sequences have zeros padded to the end until they are all of length of $L$.\n",
    "We'll focus on padding here, for simplicity.\n",
    "\n",
    "We can perform this padding manually, but Pytorch has this functionality implemented.\n",
    "As an example, let's pad the first two sequences to the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "padded_exs = pad_sequence([agnews_train[0][1], agnews_train[1][1]])\n",
    "print(\"First sequence padded: {}\".format(padded_exs[:,0]))\n",
    "print(\"First sequence length: {}\".format(len(padded_exs[:,0])))\n",
    "print(\"Second sequence padded: {}\".format(padded_exs[:,1]))\n",
    "print(\"Second sequence length: {}\".format(len(padded_exs[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although originally of unequal lengths, both sequences are now the same length, with the shorter one padded with zeros.\n",
    "\n",
    "We'd like the `DataLoader` to perform this padding operation as part of its batching process, as this will allow us to effectively combine varying-length sequences in the same input tensor.\n",
    "Fortunately, `Dataloader`s let us override the default batching behavior with the `collate_fn` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def collator(batch):\n",
    "    labels = torch.tensor([example[0] for example in batch])\n",
    "    sentences = [example[1] for example in batch]\n",
    "    data = pad_sequence(sentences)\n",
    "    \n",
    "    return [data, labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our collator padding our sequences, we can create our `DataLoader`s.\n",
    "One last thing we need to do is choose a batch size for our `DataLoader`.\n",
    "This may be something you have to play around with.\n",
    "Too big and you may exceed your system's memory; too small and training may take longer (especially on CPU).\n",
    "Batch size also tends to influence training dynamics and model generalization.\n",
    "Fiddle around and see what works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(agnews_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator)\n",
    "test_loader = torch.utils.data.DataLoader(agnews_test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Word Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try out the Simple Word Embedding Model (SWEM) that we built in Notebook 4A on the AG News dataset.\n",
    "Unlike before though, instead of loading pre-trained embeddings, let's learn the embeddings from scratch.\n",
    "Before we begin, it will be helpful to define a few more hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(agnews_train.get_vocab())\n",
    "EMBED_DIM = 100\n",
    "HIDDEN_DIM = 64\n",
    "NUM_OUTPUTS = len(agnews_train.get_labels())\n",
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we're going to organize our model as a `nn.Module`.\n",
    "Instead of assuming the input is already an embedding, we're going to make learning the embedding as part of our model.\n",
    "We do this by using `nn.Embedding` to perform an embedding look-up at the beginning of our forward pass.\n",
    "Once we've done the look up, we'll have a minibatch of embedded sequences of dimension $L \\times$ `BATCH_SIZE` $\\times$ `EMBED_DIM`.\n",
    "For SWEM, remember, we take the mean&ast; across the length dimension to get an average embedding for the sequence.\n",
    "\n",
    "<font size=\"1\"> \n",
    "&ast;Note: Technically we should only take the mean across the embeddings at the positions corresponding to \"real\" words in our input, and not for the zero paddings we artificially added.\n",
    "This can be done by generating a binary mask while doing the padding to track the \"real\" words in the input.\n",
    "Ultimately though, this refinement doesn't have much impact on the results for this particular task, so we omit it for simplicity.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SWEM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_dim, num_outputs):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(embedding_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        embed_mean = torch.mean(embed, dim=0)\n",
    "        \n",
    "        h = self.fc1(embed_mean)\n",
    "        h = F.relu(h)\n",
    "        h = self.fc2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model defined, we can instantiate, train, and evaluate.\n",
    "Try doing so below!\n",
    "Because of the way we organized our model as an `nn.Module` and our data pipeline with a `DataLoader`, you should be able to use much of the same code as we have in other examples.\n",
    "\n",
    "<font size=\"1\"> \n",
    "Note: Depending on you system, training may take up to a few hours, depending on how many training epochs you set.\n",
    "To see results sooner, you can train for less iterations, but perhaps at the cost of final accuracy.\n",
    "On the other hand, using a GPU (and GPU-enabled PyTorch) should enable full training in a couple minutes.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SWEM takes a mean over the time dimension, which means we're losing any information about the order of the data sequence.\n",
    "How detrimental is this for document topic classification?\n",
    "Modify the SWEM model to use an RNN instead.\n",
    "Once you get an RNN working, try a GRU and LSTM as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Answer:\n",
    "\n",
    "1\\. How do the RNN, GRU, and LSTM compare to SWEM for AG News topic classification?\n",
    "Are you surprised?\n",
    "What about classification might make SWEM so effective for topic classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[Your answer here]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. How many learnable parameters do each of the models you've trained have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[Your answer here]`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "23574fd47338620f5fa9034c1801f73296c9ae358dd5ea70f96243aff333c802"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
